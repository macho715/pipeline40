diff --git a/CHANGELOG.md b/CHANGELOG.md
index b38f5228d51fbf68cd88f7b19903df44e55ebabb..f2849bb765a79b6c44143918c683096fc614106e 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -41,50 +41,58 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0

 - **Files Created**:
   - `tests/test_stage3_total_sqm.py`: í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (8ê°œ í…ŒìŠ¤íŠ¸, ëª¨ë‘ í†µê³¼)

 - **Benefits**:
   - **ì ì¬ íš¨ìœ¨ ë¶„ì„**: ì‹¤ì œ ì ì¬ ê°€ëŠ¥í•œ ì´ ë©´ì  ê³„ì‚°
   - **ì¬ì‚¬ìš©ì„±**: core.data_parser í™œìš©ìœ¼ë¡œ ì½”ë“œ ì¤‘ë³µ ì œê±°
   - **ì •í™•ë„**: ê°œì„ ëœ Stack_Status íŒŒì‹± ë¡œì§ ì‚¬ìš©
   - **ì°½ê³  ê³µê°„ ê³„íš**: Total sqm ê¸°ë°˜ ì‹¤ì œ ì‚¬ìš© ê³µê°„ ì¶”ì 
   - **ì¤‘ì•™ ê´€ë¦¬**: core ëª¨ë“ˆì—ì„œ í—¤ë” ìˆœì„œ ë° íŒŒì‹± ë¡œì§ ì¼ê´„ ê´€ë¦¬

 - **Test Results**:
   - Stack_Status íŒŒì‹±: "X2" â†’ 2, "Stackable / 3" â†’ 3, "Not stackable" â†’ 0
   - Total sqm ê³„ì‚°: SQM=2.5, PKG=10 â†’ 25.0
   - ì—£ì§€ ì¼€ì´ìŠ¤: Pkg=0, SQM=None â†’ None
   - ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ (8/8)

 - **Example Usage**:
   ```python
   # Stage 3 í†µí•©_ì›ë³¸ë°ì´í„°_Fixed ì‹œíŠ¸
   # ... | SQM | Stack_Status | Total sqm | ...
   # ... | 9.84 | 2 | 98.40 | ...  (SQM=9.84, PKG=10)
   # ... | 5.20 | 3 | 52.00 | ...  (SQM=5.20, PKG=10)
   ```

+### ğŸ› Fixed
+
+- Stage 3 Excel ì €ì¥ ì‹œ `Stack_Status`, `Total sqm` ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ë˜ ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
+  - `scripts/stage3_report/report_generator.py`ì—ì„œ ëª¨ë“  ì‹œíŠ¸ë¥¼ ë‹¨ì¼ `ExcelWriter`
+    ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œ ê¸°ë¡í•˜ë„ë¡ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.
+  - ê¸°ì¡´ì—ëŠ” DataFrameì— ì‹ ê·œ ì»¬ëŸ¼ì´ ì¡´ì¬í–ˆìœ¼ë‚˜, ë‹«íŒ writer ì¬ì‚¬ìš©ìœ¼ë¡œ ì €ì¥ ë‹¨ê³„ì—ì„œ
+    ì»¬ëŸ¼ì´ ë¹ ì§€ëŠ” í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
+
 ## [4.0.21] - 2025-10-23

 ### âœ¨ Added

 #### Core ëª¨ë“ˆì— ë°ì´í„° íŒŒì‹± ìœ í‹¸ë¦¬í‹° ì¶”ê°€
 - **Problem**: Stack_Status íŒŒì‹± ë¡œì§ì´ Stage 2ì—ë§Œ ì¡´ì¬í•˜ì—¬ ì¬ì‚¬ìš© ë¶ˆê°€
   - Stageë³„ ì¤‘ë³µ ì½”ë“œ ë°œìƒ ìœ„í—˜
   - ê°œì„ ëœ íŒŒì‹± ë¡œì§ì´ ì¼ë¶€ Stageì—ë§Œ ì ìš©
   - ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€: ê° Stageë³„ë¡œ ë³„ë„ êµ¬í˜„ í•„ìš”

 - **Solution**: Core ëª¨ë“ˆì— data_parser.py ì¶”ê°€
   - **ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬**: ëª¨ë“  Stageì—ì„œ `from core.data_parser import parse_stack_status` ì‚¬ìš©
   - **ê°œì„ ëœ íŒŒì‹± ë¡œì§**: í•˜ì¤‘ í‘œê¸° ì œê±°, ìŠ¬ë˜ì‹œ íŒ¨í„´, ì–‘ë°©í–¥ X íŒ¨í„´ ì§€ì›
   - **í•˜ìœ„ í˜¸í™˜ì„±**: ê¸°ì¡´ stack_and_sqm.pyëŠ” core ëª¨ë“ˆë¡œ ìœ„ì„í•˜ì—¬ ìœ ì§€

 - **Implementation Details**:
   - `scripts/core/data_parser.py`: ìƒˆë¡œìš´ ë°ì´í„° íŒŒì‹± ëª¨ë“ˆ ìƒì„±
   - `_strip_weights()`: í•˜ì¤‘ í‘œê¸°(600kg/m2, kg/ã¡ ë“±) ì œê±° í•¨ìˆ˜
   - `parse_stack_status()`: ê°œì„ ëœ Stack_Status íŒŒì‹± ë¡œì§
   - `calculate_sqm()`, `convert_mm_to_cm()`: í–¥í›„ í™•ì¥ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
   - `scripts/core/__init__.py`: data_parser ëª¨ë“ˆ export ì¶”ê°€

 - **Files Created**:
   - `scripts/core/data_parser.py`: ë°ì´í„° íŒŒì‹± ìœ í‹¸ë¦¬í‹° (ì•½ 200ì¤„)
   - `tests/test_data_parser.py`: í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (ì•½ 150ì¤„)
diff --git a/scripts/stage3_report/report_generator.py b/scripts/stage3_report/report_generator.py
index bcb2dd14e88d7463f80a1eaf845b46d7216b890e..609933fba313b4d2bbefe0151fcd6d824544b6e5 100644
--- a/scripts/stage3_report/report_generator.py
+++ b/scripts/stage3_report/report_generator.py
@@ -860,82 +860,148 @@ class CorrectedWarehouseIOCalculator:
                     except Exception as e:
                         logger.warning(f"ì…ê³  ê³„ì‚° ì˜¤ë¥˜ (Row {idx}, Warehouse {warehouse}): {e}")
                         continue

         #  1. warehouse_transfersì— Year_Month í‚¤ ì£¼ì…
         for transfer in warehouse_transfers:
             transfer["Year_Month"] = transfer["transfer_date"].strftime("%Y-%m")

         logger.info(
             f" ìˆ˜ì •ëœ ì°½ê³  ì…ê³  ê³„ì‚° ì™„ë£Œ: {total_inbound}ê±´ (ì°½ê³ ê°„ ì´ë™ {len(warehouse_transfers)}ê±´ ë³„ë„)"
         )

         return {
             "total_inbound": total_inbound,
             "by_warehouse": by_warehouse,
             "by_month": by_month,
             "inbound_items": inbound_items,
             "warehouse_transfers": warehouse_transfers,
         }

     def _calculate_warehouse_inbound_vectorized(self, df: pd.DataFrame) -> Dict:
         """ë²¡í„°í™”ëœ ì°½ê³  ì…ê³  ê³„ì‚° (PATCH.MD ì „ëµ)"""
         logger.info(" Vectorized ì°½ê³  ì…ê³  ê³„ì‚° ì‹œì‘")

         # 1. ì°½ê³  ì»¬ëŸ¼ì„ meltí•˜ì—¬ ë²¡í„°í™” ì²˜ë¦¬
-        wh_df = df.melt(
-            id_vars=["Pkg"],
+        df_with_index = df.reset_index().rename(columns={"index": "Row_ID"})
+        wh_df = df_with_index.melt(
+            id_vars=["Row_ID", "Pkg"],
             value_vars=self.warehouse_columns,
             var_name="Warehouse",
             value_name="Inbound_Date",
         )
-        wh_df = wh_df[wh_df["Inbound_Date"].notna()]
+        wh_df = wh_df[wh_df["Inbound_Date"].notna()].copy()
+        wh_df["Row_ID"] = wh_df["Row_ID"].apply(
+            lambda value: int(value)
+            if isinstance(value, (int, np.integer))
+            else value
+        )
         wh_df["Inbound_Date"] = pd.to_datetime(wh_df["Inbound_Date"])
         wh_df["Year_Month"] = wh_df["Inbound_Date"].dt.strftime("%Y-%m")
         wh_df["Pkg_Quantity"] = wh_df["Pkg"].fillna(1).clip(lower=1).astype(int)

         # 2. ì°½ê³ ê°„ ì´ë™ ê°ì§€ (ë²¡í„°í™”)
-        df["transfers"] = df.apply(self._detect_warehouse_transfers, axis=1)
-        transfers_flat = pd.DataFrame([t for transfers in df["transfers"] for t in transfers])
+        df_with_transfers = df.copy()
+        df_with_transfers["transfers"] = df_with_transfers.apply(
+            self._detect_warehouse_transfers, axis=1
+        )
+        transfers_flat = pd.DataFrame(
+            [t for transfers in df_with_transfers["transfers"] for t in transfers]
+        )

         if not transfers_flat.empty:
-            transfers_flat["Year_Month"] = transfers_flat["transfer_date"].dt.strftime("%Y-%m")
-            # ì°½ê³ ê°„ ì´ë™ ëª©ì ì§€ ì œì™¸
-            transfer_dest = transfers_flat["to_warehouse"].unique()
-            wh_df = wh_df[~wh_df["Warehouse"].isin(transfer_dest)]
+            if "Row_ID" in transfers_flat.columns:
+                transfers_flat["Row_ID"] = transfers_flat["Row_ID"].apply(
+                    lambda value: int(value)
+                    if isinstance(value, (int, np.integer))
+                    else value
+                )
+            transfers_flat["transfer_date"] = pd.to_datetime(
+                transfers_flat["transfer_date"]
+            )
+            transfers_flat["Year_Month"] = transfers_flat["transfer_date"].dt.strftime(
+                "%Y-%m"
+            )

-        # 3. ì§‘ê³„ (ë²¡í„°í™”)
-        by_month_wh = (
-            wh_df.groupby(["Year_Month", "Warehouse"])["Pkg_Quantity"].sum().unstack(fill_value=0)
+            transfer_destinations = transfers_flat.rename(
+                columns={
+                    "to_warehouse": "Warehouse",
+                    "transfer_date": "Inbound_Date",
+                    "pkg_quantity": "Transfer_Quantity",
+                }
+            )
+            transfer_destinations = transfer_destinations[
+                ["Row_ID", "Warehouse", "Inbound_Date", "Transfer_Quantity"]
+            ]
+
+            wh_df = wh_df.merge(
+                transfer_destinations,
+                on=["Row_ID", "Warehouse", "Inbound_Date"],
+                how="left",
+            )
+            wh_df["Transfer_Quantity"] = (
+                wh_df["Transfer_Quantity"].fillna(0).astype(int)
+            )
+        else:
+            wh_df["Transfer_Quantity"] = 0
+
+        # ì°½ê³ ê°„ ì´ë™ ëª©ì ì§€ ìˆ˜ëŸ‰ ì œì™¸
+        wh_df["External_Quantity"] = (
+            wh_df["Pkg_Quantity"] - wh_df["Transfer_Quantity"]
+        ).clip(lower=0)
+        external_wh_df = wh_df[wh_df["External_Quantity"] > 0].copy()
+        external_wh_df["Pkg_Quantity"] = (
+            external_wh_df["External_Quantity"].astype(int)
         )
+        external_wh_df.drop(columns=["Transfer_Quantity", "External_Quantity", "Pkg"], inplace=True)
+
+        # 3. ì§‘ê³„ (ë²¡í„°í™”)
+        if external_wh_df.empty:
+            by_month_wh = pd.DataFrame(columns=self.warehouse_columns)
+        else:
+            by_month_wh = (
+                external_wh_df.groupby(["Year_Month", "Warehouse"])["Pkg_Quantity"].sum()
+                .unstack(fill_value=0)
+            )
         by_warehouse = by_month_wh.sum(axis=0).to_dict()
         by_month = by_month_wh.sum(axis=1).to_dict()
-        total_inbound = wh_df["Pkg_Quantity"].sum()
+        total_inbound = int(external_wh_df["Pkg_Quantity"].sum())

         # 4. ê²°ê³¼ êµ¬ì„±
-        inbound_items = wh_df.to_dict("records")
-        warehouse_transfers = transfers_flat.to_dict("records") if not transfers_flat.empty else []
+        if external_wh_df.empty:
+            inbound_items: List[Dict] = []
+        else:
+            inbound_items = external_wh_df.rename(columns={"Row_ID": "Item_ID"}).to_dict("records")
+
+        for item in inbound_items:
+            if isinstance(item.get("Item_ID"), (int, np.integer)):
+                item["Item_ID"] = int(item["Item_ID"])
+            item["Inbound_Type"] = "external_arrival"
+
+        warehouse_transfers = (
+            transfers_flat.to_dict("records") if not transfers_flat.empty else []
+        )

         logger.info(f" Vectorized ì°½ê³  ì…ê³  ê³„ì‚° ì™„ë£Œ: {total_inbound}ê±´")

         return {
             "total_inbound": total_inbound,
             "by_warehouse": by_warehouse,
             "by_month": by_month,
             "inbound_items": inbound_items,
             "warehouse_transfers": warehouse_transfers,
         }

     def _calculate_warehouse_inbound_parallel(self, df: pd.DataFrame) -> Dict:
         """ë³‘ë ¬ ì²˜ë¦¬ëœ ì°½ê³  ì…ê³  ê³„ì‚°"""
         logger.info(" Parallel ì°½ê³  ì…ê³  ê³„ì‚° ì‹œì‘")

         try:
             from multiprocessing import Pool, cpu_count
             import numpy as np

             n_cores = min(cpu_count(), 4)  # ìµœëŒ€ 4ì½”ì–´ë¡œ ì œí•œ
             chunks = np.array_split(df, n_cores)

             with Pool(n_cores) as pool:
                 results = pool.starmap(
                     self._process_chunk_inbound,
@@ -1468,126 +1534,137 @@ class CorrectedWarehouseIOCalculator:
         transfers = []

         # ì£¼ìš” ì°½ê³ ê°„ ì´ë™ íŒ¨í„´ë“¤
         warehouse_pairs = [
             ("DSV Indoor", "DSV Al Markaz"),
             ("DSV Indoor", "DSV Outdoor"),
             ("DSV Al Markaz", "DSV Outdoor"),
             ("AAA Storage", "DSV Al Markaz"),
             ("AAA Storage", "DSV Indoor"),
             ("DSV Indoor", "MOSB"),
             ("DSV Al Markaz", "MOSB"),
         ]

         for from_wh, to_wh in warehouse_pairs:
             from_date = pd.to_datetime(row.get(from_wh), errors="coerce")
             to_date = pd.to_datetime(row.get(to_wh), errors="coerce")

             if (
                 pd.notna(from_date) and pd.notna(to_date) and from_date.date() == to_date.date()
             ):  # ë™ì¼ ë‚ ì§œ ì´ë™

                 #  ì¶”ê°€: ë…¼ë¦¬ì  ê²€ì¦
                 if self._validate_transfer_logic(from_wh, to_wh, from_date, to_date):
                     transfers.append(
                         {
+                            "Row_ID": int(row.name)
+                            if isinstance(row.name, (int, np.integer))
+                            else row.name,
                             "from_warehouse": from_wh,
                             "to_warehouse": to_wh,
                             "transfer_date": from_date,
                             "pkg_quantity": self._get_pkg_quantity(row),
                             "transfer_type": "warehouse_to_warehouse",
                             "Year_Month": from_date.strftime("%Y-%m"),  #  Year_Month í‚¤ ì¶”ê°€
                         }
                     )

         return transfers

     def _vectorized_detect_warehouse_transfers_batch(self, df: pd.DataFrame) -> pd.DataFrame:
         """ì™„ì „ ë²¡í„°í™”ëœ ì°½ê³ ê°„ ì´ë™ ê°ì§€ (v4.1 ì „ëµ)"""
         logger.info(" Vectorized ì°½ê³ ê°„ ì´ë™ ê°ì§€ ì‹œì‘")

         transfers_list = []
         warehouse_pairs = [
             ("DSV Indoor", "DSV Al Markaz"),
             ("DSV Indoor", "DSV Outdoor"),
             ("DSV Al Markaz", "DSV Outdoor"),
             ("AAA Storage", "DSV Al Markaz"),
             ("AAA Storage", "DSV Indoor"),
             ("DSV Indoor", "MOSB"),
             ("DSV Al Markaz", "MOSB"),
         ]

         for from_wh, to_wh in warehouse_pairs:
             if from_wh in df.columns and to_wh in df.columns:
                 # ë²¡í„°í™”ëœ ë‚ ì§œ ë³€í™˜
                 from_date = pd.to_datetime(df[from_wh], errors="coerce")
                 to_date = pd.to_datetime(df[to_wh], errors="coerce")

                 # ë™ì¼ ë‚ ì§œ ì´ë™ ë§ˆìŠ¤í¬
                 same_date_mask = (
                     from_date.notna() & to_date.notna() & (from_date.dt.date == to_date.dt.date)
                 )

                 if same_date_mask.any():
                     # ìœ íš¨í•œ ì´ë™ë§Œ í•„í„°ë§
                     valid_mask = same_date_mask & self._validate_transfer_logic_vectorized(
                         from_wh, to_wh, from_date, to_date, df
                     )

                     if valid_mask.any():
                         # ì´ë™ ë°ì´í„° ìƒì„±
                         transfer_df = df[valid_mask].copy()
+                        transfer_df["Row_ID"] = transfer_df.index
                         transfer_df["from_warehouse"] = from_wh
                         transfer_df["to_warehouse"] = to_wh
                         transfer_df["transfer_date"] = from_date[valid_mask]
                         transfer_df["pkg_quantity"] = self._get_pkg_quantity_vectorized(transfer_df)
                         transfer_df["transfer_type"] = "warehouse_to_warehouse"
                         transfer_df["Year_Month"] = transfer_df["transfer_date"].dt.strftime(
                             "%Y-%m"
                         )

                         transfers_list.append(
                             transfer_df[
                                 [
+                                    "Row_ID",
                                     "from_warehouse",
                                     "to_warehouse",
                                     "transfer_date",
                                     "pkg_quantity",
                                     "transfer_type",
                                     "Year_Month",
                                 ]
                             ]
                         )

         if transfers_list:
             result = pd.concat(transfers_list, ignore_index=True)
             logger.info(f" Vectorized ì°½ê³ ê°„ ì´ë™ ê°ì§€ ì™„ë£Œ: {len(result)}ê±´")
+            result["Row_ID"] = result["Row_ID"].apply(
+                lambda value: int(value)
+                if isinstance(value, (int, np.integer))
+                else value
+            )
             return result
         else:
             logger.info(" Vectorized ì°½ê³ ê°„ ì´ë™ ê°ì§€ ì™„ë£Œ: 0ê±´")
             return pd.DataFrame(
                 columns=[
+                    "Row_ID",
                     "from_warehouse",
                     "to_warehouse",
                     "transfer_date",
                     "pkg_quantity",
                     "transfer_type",
                     "Year_Month",
                 ]
             )

     def _validate_transfer_logic_vectorized(self, from_wh, to_wh, from_date, to_date, df):
         """ë²¡í„°í™”ëœ ì´ë™ ë¡œì§ ê²€ì¦"""
         # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ì¦
         from_priority = pd.Series(self.location_priority.get(from_wh, 99), index=df.index)
         to_priority = pd.Series(self.location_priority.get(to_wh, 99), index=df.index)

         # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²½ìš°ë§Œ í—ˆìš© (ë‚®ì€ ìˆ«ì = ë†’ì€ ìš°ì„ ìˆœìœ„)
         priority_valid = from_priority > to_priority

         # íŠ¹ë³„ í—ˆìš© ìŒë“¤
         special_pairs = [
             ("DSV Indoor", "DSV Al Markaz"),
             ("AAA Storage", "DSV Al Markaz"),
             ("DSV Outdoor", "MOSB"),
         ]
         special_valid = pd.Series(False, index=df.index)
@@ -3244,50 +3321,55 @@ class HVDCExcelReporterFinal:

         # ì‹œíŠ¸ 1: ì°½ê³ _ì›”ë³„_ì…ì¶œê³  (Multi-Level Header, 17ì—´ - ëˆ„ê³„ í¬í•¨)
         warehouse_monthly = self.create_warehouse_monthly_sheet(stats)
         warehouse_monthly_with_headers = self.create_multi_level_headers(
             warehouse_monthly, "warehouse"
         )

         # ì‹œíŠ¸ 2: í˜„ì¥_ì›”ë³„_ì…ê³ ì¬ê³  (Multi-Level Header, 9ì—´)
         site_monthly = self.create_site_monthly_sheet(stats)
         site_monthly_with_headers = self.create_multi_level_headers(site_monthly, "site")

         # ì‹œíŠ¸ 3: Flow_Code_ë¶„ì„
         flow_analysis = self.create_flow_analysis_sheet(stats)

         # ì‹œíŠ¸ 4: ì „ì²´_íŠ¸ëœì­ì…˜_ìš”ì•½
         transaction_summary = self.create_transaction_summary_sheet(stats)

         # ì‹œíŠ¸ 5: KPI_ê²€ì¦_ê²°ê³¼ (ìˆ˜ì • ë²„ì „)
         kpi_validation_df = pd.DataFrame.from_dict(kpi_validation, orient="index")
         kpi_validation_df.reset_index(inplace=True)
         kpi_validation_df.columns = ["KPI", "Status", "Value", "Threshold"]

         # ì‹œíŠ¸ 6: ì›ë³¸_ë°ì´í„°_ìƒ˜í”Œ (ì²˜ìŒ 1000ê±´)
         sample_data = stats["processed_data"].head(1000)

+        # Stage 3 SQM ê´€ë ¨ ì‹œíŠ¸ ì‚¬ì „ ê³„ì‚°
+        sqm_cumulative_sheet = self.create_sqm_cumulative_sheet(stats)
+        sqm_invoice_sheet = self.create_sqm_invoice_sheet(stats)
+        sqm_pivot_sheet = self.create_sqm_pivot_sheet(stats)
+
         #  FIX: ì›ë³¸ ë°ì´í„° ì‹œíŠ¸ë“¤ (ì»¬ëŸ¼ ë³´ì¡´)
         hitachi_original = stats["processed_data"][
             stats["processed_data"]["Vendor"] == "HITACHI"
         ].copy()
         siemens_original = stats["processed_data"][
             stats["processed_data"]["Vendor"] == "SIMENSE"
         ].copy()
         combined_original = stats["processed_data"].copy()

         #  ê²€ì¦: AAA Storage ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
         print(f"\n ìµœì¢… ë°ì´í„° ì»¬ëŸ¼ ê²€ì¦:")
         for data_name, data_df in [
             ("HITACHI", hitachi_original),
             ("SIEMENS", siemens_original),
             ("í†µí•©", combined_original),
         ]:
             if "AAA Storage" in data_df.columns:
                 aaa_count = data_df["AAA Storage"].notna().sum()
                 print(f"    {data_name} - AAA Storage: {aaa_count}ê±´")
             else:
                 print(f"    {data_name} - AAA Storage: ì»¬ëŸ¼ ì—†ìŒ")

         #  ê²€ì¦: Status_Location_YearMonth ì»¬ëŸ¼ í™•ì¸
         if "Status_Location_YearMonth" in combined_original.columns:
             print(f"    Status_Location_YearMonth ì»¬ëŸ¼ í¬í•¨")
@@ -3308,65 +3390,50 @@ class HVDCExcelReporterFinal:
             else:
                 print(f"    {col}: ì»¬ëŸ¼ ì—†ìŒ")

         #  FIX: ì „ì²´ ë°ì´í„°ëŠ” CSVë¡œë„ ì €ì¥ (ë°±ì—…ìš©)
         hitachi_original.to_csv(
             self.report_output_dir / "HITACHI_ì›ë³¸ë°ì´í„°_FULL_fixed.csv",
             index=False,
             encoding="utf-8-sig",
         )
         siemens_original.to_csv(
             self.report_output_dir / "SIEMENS_ì›ë³¸ë°ì´í„°_FULL_fixed.csv",
             index=False,
             encoding="utf-8-sig",
         )
         combined_original.to_csv(
             self.report_output_dir / "í†µí•©_ì›ë³¸ë°ì´í„°_FULL_fixed.csv",
             index=False,
             encoding="utf-8-sig",
         )

         # Excel íŒŒì¼ ìƒì„± (ìˆ˜ì • ë²„ì „)
         excel_filename = (
             self.report_output_dir
             / f"HVDC_ì…ê³ ë¡œì§_ì¢…í•©ë¦¬í¬íŠ¸_{self.timestamp}_v3.0-corrected.xlsx"
         )
-        with pd.ExcelWriter(excel_filename, engine="xlsxwriter") as writer:
-            warehouse_monthly_with_headers.to_excel(
-                writer, sheet_name="ì°½ê³ _ì›”ë³„_ì…ì¶œê³ ", index=True
-            )
-            site_monthly_with_headers.to_excel(writer, sheet_name="í˜„ì¥_ì›”ë³„_ì…ê³ ì¬ê³ ", index=True)
-            flow_analysis.to_excel(writer, sheet_name="Flow_Code_ë¶„ì„", index=False)
-            transaction_summary.to_excel(writer, sheet_name="ì „ì²´_íŠ¸ëœì­ì…˜_ìš”ì•½", index=False)
-            kpi_validation_df.to_excel(writer, sheet_name="KPI_ê²€ì¦_ê²°ê³¼", index=False)
-            sqm_cumulative_sheet = self.create_sqm_cumulative_sheet(stats)
-            sqm_cumulative_sheet.to_excel(writer, sheet_name="SQM_ëˆ„ì ì¬ê³ ", index=False)
-            sqm_invoice_sheet = self.create_sqm_invoice_sheet(stats)
-            sqm_invoice_sheet.to_excel(writer, sheet_name="SQM_Invoiceê³¼ê¸ˆ", index=False)
-            sqm_pivot_sheet = self.create_sqm_pivot_sheet(stats)
-            sqm_pivot_sheet.to_excel(writer, sheet_name="SQM_í”¼ë²—í…Œì´ë¸”", index=False)
-            sample_data.to_excel(writer, sheet_name="ì›ë³¸_ë°ì´í„°_ìƒ˜í”Œ", index=False)
         # âœ… Stage 3 í—¤ë”ëª… ì •ê·œí™” ë° í‘œì¤€ ìˆœì„œ ì ìš©
         logger.info(" í†µí•©_ì›ë³¸ë°ì´í„°_Fixed ì‹œíŠ¸ ìƒì„± - ìœ ì—°í•œ í—¤ë” ê²€ìƒ‰ ë° í‘œì¤€ ìˆœì„œ ì ìš©")

         # HITACHI ë°ì´í„° ì²˜ë¦¬
         hitachi_normalized = normalize_header_names_for_stage3(hitachi_original)
         hitachi_reordered = reorder_dataframe_columns(
             hitachi_normalized, is_stage2=False, use_semantic_matching=True
         )

         # SIEMENS ë°ì´í„° ì²˜ë¦¬
         siemens_normalized = normalize_header_names_for_stage3(siemens_original)
         siemens_reordered = reorder_dataframe_columns(
             siemens_normalized, is_stage2=False, use_semantic_matching=True
         )

         # í†µí•© ë°ì´í„° ì²˜ë¦¬
         combined_normalized = normalize_header_names_for_stage3(combined_original)

         # âœ… Stage 3 ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€ (í†µí•© ë°ì´í„°ì—ë§Œ)
         logger.info("\n[INFO] Stage 3 ì‹ ê·œ ì»¬ëŸ¼ ê³„ì‚° ì¤‘...")

         # Stack_Status ê³„ì‚°
         combined_normalized["Stack_Status"] = _calculate_stack_status(combined_normalized, "Stack")
         stack_parsed = combined_normalized["Stack_Status"].notna().sum()
         logger.info(f"  - Stack_Status íŒŒì‹± ì™„ë£Œ: {stack_parsed}ê°œ")
@@ -3434,91 +3501,117 @@ class HVDCExcelReporterFinal:
             logger.info(f"  - Total sqm ìœ íš¨ê°’ ê°œìˆ˜: {total_sqm_count}")
         if "Stack_Status" in combined_reordered.columns:
             stack_status_count = combined_reordered["Stack_Status"].notna().sum()
             logger.info(f"  - Stack_Status ìœ íš¨ê°’ ê°œìˆ˜: {stack_status_count}")

         # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì§ì „ ìµœì¢… ê²€ì¦
         logger.info(f"\n[DEBUG] Excel ì €ì¥ ì§ì „ ìµœì¢… ê²€ì¦:")
         logger.info(f"  - combined_reordered ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
         logger.info(f"  - ë§ˆì§€ë§‰ 5ê°œ ì»¬ëŸ¼: {list(combined_reordered.columns[-5:])}")

         # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì „ ì»¬ëŸ¼ëª… ê²€ì¦
         logger.info(f"\n[DEBUG] Excel ì €ì¥ ì „ ì»¬ëŸ¼ëª… ê²€ì¦:")
         logger.info(f"  - Total sqm ì»¬ëŸ¼ëª…: {repr('Total sqm')}")
         logger.info(f"  - Stack_Status ì»¬ëŸ¼ëª…: {repr('Stack_Status')}")
         logger.info(f"  - Total sqm in columns: {'Total sqm' in combined_reordered.columns}")
         logger.info(f"  - Stack_Status in columns: {'Stack_Status' in combined_reordered.columns}")

         # ğŸ” ë””ë²„ê·¸: ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ëª… í™•ì¸
         problem_cols = []
         for col in combined_reordered.columns:
             if any(char in col for char in ["\n", "\r", "\t", "\x00"]):
                 problem_cols.append(f"'{col}' (contains special chars)")
         if problem_cols:
             logger.warning(f"[WARN] ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ëª…: {problem_cols}")

-        #  FIX: ìˆ˜ì •ëœ ì›ë³¸ ë°ì´í„° ì‹œíŠ¸ë“¤ (í‘œì¤€ í—¤ë” ìˆœì„œ ì ìš©)
-        hitachi_reordered.to_excel(writer, sheet_name="HITACHI_ì›ë³¸ë°ì´í„°_Fixed", index=False)
-        siemens_reordered.to_excel(writer, sheet_name="SIEMENS_ì›ë³¸ë°ì´í„°_Fixed", index=False)
+        with pd.ExcelWriter(excel_filename, engine="xlsxwriter") as writer:
+            warehouse_monthly_with_headers.to_excel(
+                writer, sheet_name="ì°½ê³ _ì›”ë³„_ì…ì¶œê³ ", index=True
+            )
+            site_monthly_with_headers.to_excel(
+                writer, sheet_name="í˜„ì¥_ì›”ë³„_ì…ê³ ì¬ê³ ", index=True
+            )
+            flow_analysis.to_excel(writer, sheet_name="Flow_Code_ë¶„ì„", index=False)
+            transaction_summary.to_excel(writer, sheet_name="ì „ì²´_íŠ¸ëœì­ì…˜_ìš”ì•½", index=False)
+            kpi_validation_df.to_excel(writer, sheet_name="KPI_ê²€ì¦_ê²°ê³¼", index=False)
+            sqm_cumulative_sheet.to_excel(writer, sheet_name="SQM_ëˆ„ì ì¬ê³ ", index=False)
+            sqm_invoice_sheet.to_excel(writer, sheet_name="SQM_Invoiceê³¼ê¸ˆ", index=False)
+            sqm_pivot_sheet.to_excel(writer, sheet_name="SQM_í”¼ë²—í…Œì´ë¸”", index=False)
+            sample_data.to_excel(writer, sheet_name="ì›ë³¸_ë°ì´í„°_ìƒ˜í”Œ", index=False)

-        # ğŸ” ë””ë²„ê·¸: combined_reordered ì €ì¥ ì „ ìµœì¢… í™•ì¸
-        logger.info(f"\n[DEBUG] combined_reordered Excel ì €ì¥ ì§ì „:")
-        logger.info(f"  - ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
-        logger.info(
-            f"  - Total sqm ìœ„ì¹˜: {list(combined_reordered.columns).index('Total sqm') if 'Total sqm' in combined_reordered.columns else 'NOT FOUND'}"
-        )
-        logger.info(
-            f"  - Stack_Status ìœ„ì¹˜: {list(combined_reordered.columns).index('Stack_Status') if 'Stack_Status' in combined_reordered.columns else 'NOT FOUND'}"
-        )
+            #  FIX: ìˆ˜ì •ëœ ì›ë³¸ ë°ì´í„° ì‹œíŠ¸ë“¤ (í‘œì¤€ í—¤ë” ìˆœì„œ ì ìš©)
+            hitachi_reordered.to_excel(
+                writer, sheet_name="HITACHI_ì›ë³¸ë°ì´í„°_Fixed", index=False
+            )
+            siemens_reordered.to_excel(
+                writer, sheet_name="SIEMENS_ì›ë³¸ë°ì´í„°_Fixed", index=False
+            )

-        # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì „ ìµœì¢… ì»¬ëŸ¼ ê²€ì¦
-        logger.info(f"\n[DEBUG] Excel ì €ì¥ ì „ ìµœì¢… ì»¬ëŸ¼ ê²€ì¦:")
-        logger.info(f"  - combined_reordered ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
-        logger.info(f"  - Total sqm ì¡´ì¬: {'Total sqm' in combined_reordered.columns}")
-        logger.info(f"  - Stack_Status ì¡´ì¬: {'Stack_Status' in combined_reordered.columns}")
-        logger.info(
-            f"  - Total sqm ìœ„ì¹˜: {list(combined_reordered.columns).index('Total sqm') if 'Total sqm' in combined_reordered.columns else 'NOT FOUND'}"
-        )
-        logger.info(
-            f"  - Stack_Status ìœ„ì¹˜: {list(combined_reordered.columns).index('Stack_Status') if 'Stack_Status' in combined_reordered.columns else 'NOT FOUND'}"
-        )
+            # ğŸ” ë””ë²„ê·¸: combined_reordered ì €ì¥ ì „ ìµœì¢… í™•ì¸
+            logger.info(f"\n[DEBUG] combined_reordered Excel ì €ì¥ ì§ì „:")
+            logger.info(f"  - ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
+            logger.info(
+                f"  - Total sqm ìœ„ì¹˜: {list(combined_reordered.columns).index('Total sqm') if 'Total sqm' in combined_reordered.columns else 'NOT FOUND'}"
+            )
+            logger.info(
+                f"  - Stack_Status ìœ„ì¹˜: {list(combined_reordered.columns).index('Stack_Status') if 'Stack_Status' in combined_reordered.columns else 'NOT FOUND'}"
+            )

-        # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì‹œë„
-        try:
-            # Excel ì €ì¥ ì‹œ ì»¬ëŸ¼ ì œí•œ í™•ì¸
-            logger.info(f"[DEBUG] Excel ì €ì¥ ì‹œë„: {len(combined_reordered.columns)}ê°œ ì»¬ëŸ¼")
-            combined_reordered.to_excel(writer, sheet_name="í†µí•©_ì›ë³¸ë°ì´í„°_Fixed", index=False)
-            logger.info("[SUCCESS] Excel ì €ì¥ ì™„ë£Œ")
-        except Exception as e:
-            logger.error(f"[ERROR] Excel ì €ì¥ ì‹¤íŒ¨: {e}")
-            # ì»¬ëŸ¼ëª… ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì»¬ëŸ¼ëª…ì„ ì•ˆì „í•˜ê²Œ ë³€ê²½
-            safe_df = combined_reordered.copy()
-            safe_df.columns = [
-                str(col).replace(" ", "_").replace(".", "_") for col in safe_df.columns
-            ]
-            safe_df.to_excel(writer, sheet_name="í†µí•©_ì›ë³¸ë°ì´í„°_Fixed", index=False)
-            logger.info("[FALLBACK] ì•ˆì „í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ Excel ì €ì¥ ì™„ë£Œ")
+            # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì „ ìµœì¢… ì»¬ëŸ¼ ê²€ì¦
+            logger.info(f"\n[DEBUG] Excel ì €ì¥ ì „ ìµœì¢… ì»¬ëŸ¼ ê²€ì¦:")
+            logger.info(f"  - combined_reordered ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
+            logger.info(f"  - Total sqm ì¡´ì¬: {'Total sqm' in combined_reordered.columns}")
+            logger.info(f"  - Stack_Status ì¡´ì¬: {'Stack_Status' in combined_reordered.columns}")
+            logger.info(
+                f"  - Total sqm ìœ„ì¹˜: {list(combined_reordered.columns).index('Total sqm') if 'Total sqm' in combined_reordered.columns else 'NOT FOUND'}"
+            )
+            logger.info(
+                f"  - Stack_Status ìœ„ì¹˜: {list(combined_reordered.columns).index('Stack_Status') if 'Stack_Status' in combined_reordered.columns else 'NOT FOUND'}"
+            )
+
+            # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ ì‹œë„
+            try:
+                # Excel ì €ì¥ ì‹œ ì»¬ëŸ¼ ì œí•œ í™•ì¸
+                logger.info(
+                    f"[DEBUG] Excel ì €ì¥ ì‹œë„: {len(combined_reordered.columns)}ê°œ ì»¬ëŸ¼"
+                )
+                combined_reordered.to_excel(
+                    writer, sheet_name="í†µí•©_ì›ë³¸ë°ì´í„°_Fixed", index=False
+                )
+                logger.info("[SUCCESS] Excel ì €ì¥ ì™„ë£Œ")
+            except Exception as e:
+                logger.error(f"[ERROR] Excel ì €ì¥ ì‹¤íŒ¨: {e}")
+                # ì»¬ëŸ¼ëª… ë¬¸ì œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì»¬ëŸ¼ëª…ì„ ì•ˆì „í•˜ê²Œ ë³€ê²½
+                safe_df = combined_reordered.copy()
+                safe_df.columns = [
+                    str(col).replace(" ", "_").replace(".", "_")
+                    for col in safe_df.columns
+                ]
+                safe_df.to_excel(
+                    writer, sheet_name="í†µí•©_ì›ë³¸ë°ì´í„°_Fixed", index=False
+                )
+                logger.info("[FALLBACK] ì•ˆì „í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ Excel ì €ì¥ ì™„ë£Œ")

         # ğŸ” ë””ë²„ê·¸: Excel ì €ì¥ í›„ ê²€ì¦
         logger.info(f"\n[DEBUG] Excel ì €ì¥ í›„ ê²€ì¦:")
         logger.info(f"  - combined_reordered ì»¬ëŸ¼ ìˆ˜: {len(combined_reordered.columns)}")
         logger.info(f"  - 'Total sqm' ì¡´ì¬: {'Total sqm' in combined_reordered.columns}")
         logger.info(f"  - 'Stack_Status' ì¡´ì¬: {'Stack_Status' in combined_reordered.columns}")

         logger.info(f" í‘œì¤€ í—¤ë” ìˆœì„œ ì ìš© ì™„ë£Œ: {len(combined_reordered.columns)}ê°œ ì»¬ëŸ¼")

         # ì €ì¥ í›„ ê²€ì¦
         try:
             _ = pd.read_excel(excel_filename, sheet_name=0)
         except Exception as e:
             print(f" [ê²½ê³ ] ì—‘ì…€ íŒŒì¼ ì €ì¥ í›„ ì—´ê¸° ì‹¤íŒ¨: {e}")

         logger.info(f" ìµœì¢… Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {excel_filename}")
         logger.info(
             " ì›ë³¸ ì „ì²´ ë°ì´í„°ëŠ” %s ê²½ë¡œì˜ CSVë¡œë„ ì €ì¥ë¨",
             self.report_output_dir,
         )

         #  FIX: ìˆ˜ì •ì‚¬í•­ ìš”ì•½ ì¶œë ¥
         print(f"\n v3.0-corrected ìˆ˜ì •ì‚¬í•­ ìš”ì•½:")
         print(f"    1. ì°½ê³  vs í˜„ì¥ ì…ê³  ë¶„ë¦¬")
         print(f"    2. ì¶œê³  íƒ€ì´ë° ì •í™•ì„± ê°œì„ ")
diff --git a/tests/stage3/test_vectorized_inbound.py b/tests/stage3/test_vectorized_inbound.py
new file mode 100644
index 0000000000000000000000000000000000000000..071501a026ab9777ab9a05608d87a72872dc333e
--- /dev/null
+++ b/tests/stage3/test_vectorized_inbound.py
@@ -0,0 +1,46 @@
+import pandas as pd
+
+from scripts.stage3_report.report_generator import CorrectedWarehouseIOCalculator
+
+
+def test_vectorized_inbound_retains_external_arrivals_for_transfer_destinations():
+    calculator = CorrectedWarehouseIOCalculator(use_vectorized=True)
+
+    warehouse_data = {column: [None, None] for column in calculator.warehouse_columns}
+    warehouse_data["DSV Indoor"][0] = "2023-02-10"
+    warehouse_data["DSV Al Markaz"][0] = "2023-02-10"
+    warehouse_data["DSV Al Markaz"][1] = "2023-02-12"
+
+    data = {"Pkg": [10, 5]}
+    data.update(warehouse_data)
+
+    df = pd.DataFrame(data)
+
+    inbound_result = calculator._calculate_warehouse_inbound_vectorized(df)
+
+    assert inbound_result["total_inbound"] == 15
+    assert inbound_result["by_warehouse"]["DSV Indoor"] == 10
+    assert inbound_result["by_warehouse"]["DSV Al Markaz"] == 5
+
+    inbound_items = pd.DataFrame(inbound_result["inbound_items"])
+    assert not inbound_items.empty
+    assert (
+        inbound_items[
+            (inbound_items["Warehouse"] == "DSV Al Markaz")
+            & (inbound_items["Inbound_Date"] == pd.Timestamp("2023-02-10"))
+        ]
+        .empty
+    )
+
+    al_markaz_external = inbound_items[
+        (inbound_items["Warehouse"] == "DSV Al Markaz")
+        & (inbound_items["Inbound_Date"] == pd.Timestamp("2023-02-12"))
+    ]
+    assert not al_markaz_external.empty
+    assert al_markaz_external.iloc[0]["Pkg_Quantity"] == 5
+
+    warehouse_transfers = inbound_result["warehouse_transfers"]
+    assert any(
+        transfer["to_warehouse"] == "DSV Al Markaz" and transfer.get("Row_ID") == 0
+        for transfer in warehouse_transfers
+    )
