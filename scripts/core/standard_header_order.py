"""
í‘œì¤€ í—¤ë” ìˆœì„œ ì •ì˜ (Standard Header Order)

ì‹¤ì œ "í†µí•©_ì›ë³¸ë°ì´í„°_Fixed" ì‹œíŠ¸ì˜ í—¤ë” ìˆœì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ
Stage 2 ë° Stage 3 ì¶œë ¥ íŒŒì¼ì˜ ì»¬ëŸ¼ ìˆœì„œë¥¼ í†µì¼í•©ë‹ˆë‹¤.

ê¸°ì¤€: Stage 3 ë³´ê³ ì„œì˜ "í†µí•©_ì›ë³¸ë°ì´í„°_Fixed" ì‹œíŠ¸ (64ê°œ ì»¬ëŸ¼)

ì¤‘ìš”: ê¸°ì¡´ core/ ë””ë ‰í† ë¦¬ì˜ HeaderNormalizer, HeaderRegistry, SemanticMatcherë¥¼
      í™œìš©í•˜ì—¬ ìœ ì—°í•œ í—¤ë” ë§¤ì¹­ ë° ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import pandas as pd
from typing import Optional, List, Dict, Tuple, Set
import logging
import re
from difflib import SequenceMatcher

# ê¸°ì¡´ core ë¡œì§ import
from .header_normalizer import HeaderNormalizer
from .header_registry import HeaderRegistry
from .semantic_matcher import SemanticMatcher

logger = logging.getLogger(__name__)

# ì‹¤ì œ "í†µí•©_ì›ë³¸ë°ì´í„°_Fixed" ì‹œíŠ¸ ê¸°ì¤€ í‘œì¤€ í—¤ë” ìˆœì„œ (64ê°œ)
STANDARD_HEADER_ORDER = [
    # ê¸°ë³¸ ì‹ë³„ ì •ë³´
    "no.",
    "Shipment Invoice No.",
    "HVDC CODE",  # Stage 3ì—ì„œ ì¶”ê°€
    "Site",
    "EQ No",
    "Case No.",
    "Pkg",
    "Storage",
    "Description",
    # ì¹˜ìˆ˜ ì •ë³´ (SQM ê³„ì‚°ì˜ ì…ë ¥)
    "L(CM)",
    "W(CM)",
    "H(CM)",
    "CBM",
    "N.W(kgs)",
    "G.W(kgs)",
    # Stack ê´€ë ¨
    "Stack",
    "HS Code",
    "Currency",
    "Price",
    "Vessel",
    "COE",
    "POL",
    "POD",
    "ETD/ATD",
    "ETA/ATA",
    # ì°½ê³  ì •ë³´ (HeaderRegistryì— ì •ì˜ëœ ìˆœì„œ)
    "DHL WH",
    "DSV Indoor",
    "DSV Al Markaz",
    "Hauler Indoor",
    "DSV Outdoor",
    "DSV MZP",
    "HAULER",
    "JDN MZD",
    "MOSB",
    "AAA Storage",
    # ì¶”ê°€ ì°½ê³ /ì‘ì—…
    "Shifting",
    # í˜„ì¥ ì •ë³´ (ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ì»¬ëŸ¼ëª…)
    "MIR",
    "SHU",
    "AGI",
    "DAS",
    # ë©”íƒ€ë°ì´í„°
    "Source_Sheet",
    # ìƒíƒœ ì •ë³´ (Status_ë¡œ ì‹œì‘í•˜ëŠ” íŒŒìƒ ì»¬ëŸ¼)
    "Status_WAREHOUSE",
    "Status_SITE",
    "Status_Current",
    "Status_Location",
    "Status_Location_Date",
    "Status_Storage",
    # Handling ì •ë³´ (íŒŒìƒ ì»¬ëŸ¼)
    "wh_handling_legacy",  # Stage 3ì—ì„œ "wh handling"ì„ ì´ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
    "site handling",  # ê³µë°± 1ê°œ
    "total handling",
    "minus",
    "final handling",
    # SQM ë° Stack_Status (Stage 2ì—ì„œ ê³„ì‚°)
    "SQM",
    "Stack_Status",
    "Total sqm",  # Stage 3: PKG Ã— SQM Ã— Stack_Status
    # Stage 3ì—ì„œ ì¶”ê°€ë˜ëŠ” ë©”íƒ€ ì»¬ëŸ¼
    "Vendor",
    "Source_File",
    "Status_Location_YearMonth",
    "site_handling_original",
    "total_handling_original",
    "wh_handling_original",
    "FLOW_CODE",
    "FLOW_DESCRIPTION",
    "Final_Location",
    # ì…ê³ ì¼ì
    "ì…ê³ ì¼ì",
]

# Stage 2 ì „ìš© í—¤ë” ìˆœì„œ (Stage 3ì—ì„œë§Œ ì¶”ê°€ë˜ëŠ” ì»¬ëŸ¼ ì œì™¸)
STAGE2_HEADER_ORDER = [
    "no.",
    "Shipment Invoice No.",
    "SCT Ref.No",  # Stage 2ì—ë§Œ ìˆìŒ
    "Site",
    "EQ No",
    "Case No.",
    "Pkg",
    "Storage",
    "Description",
    "L(CM)",
    "W(CM)",
    "H(CM)",
    "CBM",
    "N.W(kgs)",
    "G.W(kgs)",
    "Stack",
    "HS Code",
    "Currency",
    "Price",
    "Vessel",
    "COE",
    "POL",
    "POD",
    "ETD/ATD",
    "ETA/ATA",
    # "no" ì œê±°ë¨ - 26ë²ˆì§¸ì— DHL WHì´ ì˜´
    "DHL WH",
    "DSV Indoor",
    "DSV Al Markaz",
    "Hauler Indoor",
    "DSV Outdoor",
    "DSV MZP",
    "HAULER",
    "JDN MZD",
    "MOSB",
    "AAA Storage",
    "Shifting",
    # í˜„ì¥ ì»¬ëŸ¼ (ì‹¤ì œ ì‚¬ìš© ì¤‘)
    "MIR",
    "SHU",
    "AGI",
    "DAS",
    "Source_Sheet",
    "Status_WAREHOUSE",
    "Status_SITE",
    "Status_Current",
    "Status_Location",
    "Status_Location_Date",
    "Status_Storage",
    "wh handling",  # Stage 2 ì›ë³¸
    "site  handling",  # ê³µë°± 2ê°œ - Stage 2 ì›ë³¸
    "total handling",
    "minus",
    "final handling",
    "SQM",
    "Stack_Status",
]


class FlexibleHeaderMatcher:
    """
    ìœ ì—°í•œ í—¤ë” ë§¤ì¹­ í´ë˜ìŠ¤

    ê¸°ì¡´ core ë¡œì§ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ í—¤ë” ë³€í˜•ì„ ìë™ìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.normalizer = HeaderNormalizer()
        self.registry = HeaderRegistry()
        self.semantic_matcher = SemanticMatcher()

        # ìœ ì—°í•œ ë§¤ì¹­ì„ ìœ„í•œ íŒ¨í„´ ì •ì˜
        self._init_matching_patterns()

    def _init_matching_patterns(self):
        """ë§¤ì¹­ íŒ¨í„´ ì´ˆê¸°í™”"""
        # ì¼ë°˜ì ì¸ í—¤ë” ë³€í˜• íŒ¨í„´
        self.patterns = {
            # ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ë³€í˜•
            "space_variations": [
                (r"\s+", " "),  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                (r"^\s+|\s+$", ""),  # ì•ë’¤ ê³µë°± ì œê±°
            ],
            # ëŒ€ì†Œë¬¸ì ë³€í˜•
            "case_variations": [
                (r"^no\.?$", "no.", re.IGNORECASE),
                (r"^site\s*handling$", "site handling", re.IGNORECASE),
                (r"^wh\s*handling$", "wh handling", re.IGNORECASE),
            ],
            # íŠ¹ìˆ˜ë¬¸ì ë³€í˜•
            "special_chars": [
                (r"\(", "\\("),
                (r"\)", "\\)"),
                (r"\.", "\\."),
                (r"\+", "\\+"),
                (r"\*", "\\*"),
                (r"\?", "\\?"),
                (r"\[", "\\["),
                (r"\]", "\\]"),
            ],
        }

    def normalize_header_name(self, header_name: str) -> str:
        """
        í—¤ë”ëª…ì„ ì •ê·œí™” (ê¸°ì¡´ HeaderNormalizer í™œìš©)

        Args:
            header_name: ì›ë³¸ í—¤ë”ëª…

        Returns:
            ì •ê·œí™”ëœ í—¤ë”ëª…
        """
        try:
            # ê¸°ì¡´ HeaderNormalizer ì‚¬ìš©
            normalized = self.normalizer.normalize(header_name, expand_abbreviations=False)

            # ì¶”ê°€ ì •ê·œí™” (ê³µë°± ì •ë¦¬)
            normalized = re.sub(r"\s+", " ", normalized.strip())

            return normalized
        except Exception as e:
            logger.warning(f"HeaderNormalizer ì‹¤íŒ¨, ê¸°ë³¸ ì •ê·œí™” ì‚¬ìš©: {e}")
            return header_name.strip()

    def calculate_similarity(self, str1: str, str2: str) -> float:
        """
        ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°

        Args:
            str1, str2: ë¹„êµí•  ë¬¸ìì—´

        Returns:
            ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # ì •ê·œí™”
        norm1 = self.normalize_header_name(str1)
        norm2 = self.normalize_header_name(str2)

        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if norm1 == norm2:
            return 1.0

        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if norm1.lower() == norm2.lower():
            return 0.95

        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = SequenceMatcher(None, norm1.lower(), norm2.lower()).ratio()

        # ë¶€ë¶„ ì¼ì¹˜ ë³´ë„ˆìŠ¤
        if norm1.lower() in norm2.lower() or norm2.lower() in norm1.lower():
            similarity = max(similarity, 0.8)

        return similarity

    def find_best_match(
        self, target_header: str, candidate_headers: List[str], min_similarity: float = 0.6
    ) -> Optional[Tuple[str, float]]:
        """
        ëŒ€ìƒ í—¤ë”ì— ëŒ€í•œ ìµœì  ë§¤ì¹­ì„ ì°¾ìŠµë‹ˆë‹¤

        Args:
            target_header: ë§¤ì¹­í•  ëŒ€ìƒ í—¤ë”
            candidate_headers: í›„ë³´ í—¤ë” ë¦¬ìŠ¤íŠ¸
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            (ë§¤ì¹­ëœ_í—¤ë”, ìœ ì‚¬ë„) íŠœí”Œ ë˜ëŠ” None
        """
        best_match = None
        best_similarity = 0.0

        for candidate in candidate_headers:
            similarity = self.calculate_similarity(target_header, candidate)

            if similarity > best_similarity and similarity >= min_similarity:
                best_similarity = similarity
                best_match = candidate

        if best_match:
            logger.debug(
                f"í—¤ë” ë§¤ì¹­: '{target_header}' â†’ '{best_match}' (ìœ ì‚¬ë„: {best_similarity:.3f})"
            )
            return (best_match, best_similarity)

        return None

    def semantic_match(self, header_name: str, standard_headers: List[str]) -> Optional[str]:
        """
        ì˜ë¯¸ë¡ ì  ë§¤ì¹­ì„ ì‚¬ìš©í•˜ì—¬ í—¤ë”ë¥¼ ì°¾ìŠµë‹ˆë‹¤

        Args:
            header_name: ë§¤ì¹­í•  í—¤ë”ëª…
            standard_headers: í‘œì¤€ í—¤ë” ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§¤ì¹­ëœ í‘œì¤€ í—¤ë”ëª… ë˜ëŠ” None
        """
        try:
            # HeaderRegistryì˜ ì˜ë¯¸ë¡ ì  ë§¤ì¹­ í™œìš©
            for standard_header in standard_headers:
                # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë¹„êµ
                norm_header = self.normalize_header_name(header_name)
                norm_standard = self.normalize_header_name(standard_header)

                # ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± í™•ì¸
                if self.semantic_matcher.is_semantically_similar(norm_header, norm_standard):
                    logger.debug(f"ì˜ë¯¸ë¡ ì  ë§¤ì¹­: '{header_name}' â†’ '{standard_header}'")
                    return standard_header
        except Exception as e:
            logger.warning(f"ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ì‹¤íŒ¨: {e}")

        return None


class HeaderOrderManager:
    """
    í—¤ë” ìˆœì„œ ê´€ë¦¬ í´ë˜ìŠ¤

    ê¸°ì¡´ core ë¡œì§ê³¼ ìœ ì—°í•œ ë§¤ì¹­ì„ í™œìš©í•˜ì—¬
    í—¤ë” ë§¤ì¹­ ë° ì¬ì •ë ¬ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.matcher = FlexibleHeaderMatcher()

    def match_columns_to_standard(
        self,
        current_columns: List[str],
        standard_order: List[str],
        use_semantic_matching: bool = True,
    ) -> Dict[str, str]:
        """
        í˜„ì¬ ì»¬ëŸ¼ë“¤ì„ í‘œì¤€ í—¤ë”ì™€ ë§¤ì¹­ (ìœ ì—°í•œ ê²€ìƒ‰)

        Args:
            current_columns: í˜„ì¬ DataFrameì˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            standard_order: í‘œì¤€ í—¤ë” ìˆœì„œ ë¦¬ìŠ¤íŠ¸
            use_semantic_matching: ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€

        Returns:
            {í˜„ì¬_ì»¬ëŸ¼: í‘œì¤€_ì»¬ëŸ¼} ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        """
        mapping = {}
        used_standards = set()

        # 1ë‹¨ê³„: ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ ë¨¼ì € ë§¤í•‘
        for col in current_columns:
            if col in standard_order:
                mapping[col] = col
                used_standards.add(col)
                logger.debug(f"ì •í™• ë§¤ì¹­: '{col}' â†’ '{col}'")

        # 2ë‹¨ê³„: ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
        unmapped_current = [c for c in current_columns if c not in mapping]
        available_standards = [s for s in standard_order if s not in used_standards]

        for current_col in unmapped_current:
            # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
            normalized_current = self.matcher.normalize_header_name(current_col)

            for standard_col in available_standards:
                normalized_standard = self.matcher.normalize_header_name(standard_col)

                if normalized_current == normalized_standard:
                    mapping[current_col] = standard_col
                    used_standards.add(standard_col)
                    logger.debug(f"ì •ê·œí™” ë§¤ì¹­: '{current_col}' â†’ '{standard_col}'")
                    break

        # 3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        if use_semantic_matching:
            unmapped_current = [c for c in current_columns if c not in mapping]
            available_standards = [s for s in standard_order if s not in used_standards]

            for current_col in unmapped_current:
                # ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ì‹œë„
                semantic_match = self.matcher.semantic_match(current_col, available_standards)
                if semantic_match:
                    mapping[current_col] = semantic_match
                    used_standards.add(semantic_match)
                    logger.debug(f"ì˜ë¯¸ë¡ ì  ë§¤ì¹­: '{current_col}' â†’ '{semantic_match}'")
                    continue

                # ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
                best_match = self.matcher.find_best_match(
                    current_col, available_standards, min_similarity=0.7
                )
                if best_match:
                    matched_col, similarity = best_match
                    mapping[current_col] = matched_col
                    used_standards.add(matched_col)
                    logger.debug(
                        f"ìœ ì‚¬ë„ ë§¤ì¹­: '{current_col}' â†’ '{matched_col}' (ìœ ì‚¬ë„: {similarity:.3f})"
                    )

        # ë§¤ì¹­ ê²°ê³¼ ë¡œê¹…
        total_mapped = len(mapping)
        total_current = len(current_columns)
        mapping_rate = (total_mapped / total_current * 100) if total_current > 0 else 0

        logger.info(f"í—¤ë” ë§¤ì¹­ ì™„ë£Œ: {total_mapped}/{total_current}ê°œ ({mapping_rate:.1f}%)")

        return mapping

    def reorder_dataframe(
        self,
        df: pd.DataFrame,
        is_stage2: bool = False,
        keep_unlisted: bool = True,
        use_semantic_matching: bool = True,
    ) -> pd.DataFrame:
        """
        DataFrameì˜ ì»¬ëŸ¼ì„ í‘œì¤€ ìˆœì„œë¡œ ì¬ì •ë ¬ (ìœ ì—°í•œ ê²€ìƒ‰)

        Args:
            df: ì¬ì •ë ¬í•  DataFrame
            is_stage2: Stage 2 ì¶œë ¥ì¸ ê²½ìš° True
            keep_unlisted: í‘œì¤€ ìˆœì„œì— ì—†ëŠ” ì»¬ëŸ¼ì„ ëì— ì¶”ê°€í• ì§€ ì—¬ë¶€
            use_semantic_matching: ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€

        Returns:
            ì¬ì •ë ¬ëœ DataFrame
        """
        standard_order = STAGE2_HEADER_ORDER if is_stage2 else STANDARD_HEADER_ORDER
        current_columns = list(df.columns)

        logger.info(
            f"ğŸ”„ í—¤ë” ì¬ì •ë ¬ ì‹œì‘ ({'Stage 2' if is_stage2 else 'Stage 3'}): {len(current_columns)}ê°œ ì»¬ëŸ¼"
        )

        # í—¤ë” ë§¤ì¹­ (ìœ ì—°í•œ ê²€ìƒ‰)
        mapping = self.match_columns_to_standard(
            current_columns, standard_order, use_semantic_matching=use_semantic_matching
        )

        # í‘œì¤€ ìˆœì„œì— ë§ì¶° ì»¬ëŸ¼ ì¬ì •ë ¬
        ordered_columns = []
        for std_col in standard_order:
            # ë§¤í•‘ëœ ì›ë³¸ ì»¬ëŸ¼ëª… ì°¾ê¸°
            original_col = next((k for k, v in mapping.items() if v == std_col), None)
            if original_col and original_col in df.columns:
                ordered_columns.append(original_col)

        # í‘œì¤€ ìˆœì„œì— ì—†ëŠ” ì»¬ëŸ¼ ì¶”ê°€ (ëì—)
        remaining_columns = []
        if keep_unlisted:
            remaining_columns = [c for c in current_columns if c not in ordered_columns]

        final_order = ordered_columns + remaining_columns

        logger.info(
            f"âœ… í—¤ë” ì¬ì •ë ¬ ì™„ë£Œ: {len(ordered_columns)}ê°œ í‘œì¤€ ìˆœì„œ, {len(remaining_columns)}ê°œ ì¶”ê°€ ì»¬ëŸ¼"
        )

        return df[final_order]

    def detect_header_variations(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        DataFrameì˜ í—¤ë” ë³€í˜•ì„ ê°ì§€í•©ë‹ˆë‹¤

        Args:
            df: ë¶„ì„í•  DataFrame

        Returns:
            {í‘œì¤€_í—¤ë”: [ë°œê²¬ëœ_ë³€í˜•ë“¤]} ë”•ì…”ë„ˆë¦¬
        """
        variations = {}
        current_columns = list(df.columns)

        # ëª¨ë“  í‘œì¤€ í—¤ë”ì— ëŒ€í•´ ë³€í˜• ê²€ìƒ‰
        for standard_header in STANDARD_HEADER_ORDER:
            found_variations = []

            for col in current_columns:
                similarity = self.matcher.calculate_similarity(standard_header, col)
                if similarity >= 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•œ ê²½ìš°
                    found_variations.append((col, similarity))

            if found_variations:
                # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                found_variations.sort(key=lambda x: x[1], reverse=True)
                variations[standard_header] = [var[0] for var in found_variations]

        return variations


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_manager = None


def get_header_manager() -> HeaderOrderManager:
    """HeaderOrderManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _manager
    if _manager is None:
        _manager = HeaderOrderManager()
    return _manager


def reorder_dataframe_columns(
    df: pd.DataFrame,
    is_stage2: bool = False,
    keep_unlisted: bool = True,
    use_semantic_matching: bool = True,
) -> pd.DataFrame:
    """
    DataFrameì˜ ì»¬ëŸ¼ì„ í‘œì¤€ ìˆœì„œë¡œ ì¬ì •ë ¬ (í¸ì˜ í•¨ìˆ˜)

    Args:
        df: ì¬ì •ë ¬í•  DataFrame
        is_stage2: Stage 2 ì¶œë ¥ì¸ ê²½ìš° True
        keep_unlisted: í‘œì¤€ ìˆœì„œì— ì—†ëŠ” ì»¬ëŸ¼ì„ ëì— ì¶”ê°€í• ì§€ ì—¬ë¶€
        use_semantic_matching: ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ì‚¬ìš© ì—¬ë¶€

    Returns:
        ì¬ì •ë ¬ëœ DataFrame
    """
    manager = get_header_manager()
    return manager.reorder_dataframe(
        df,
        is_stage2=is_stage2,
        keep_unlisted=keep_unlisted,
        use_semantic_matching=use_semantic_matching,
    )


def validate_sqm_stack_presence(df: pd.DataFrame) -> dict:
    """
    SQMê³¼ Stack_Status ì»¬ëŸ¼ì˜ ì¡´ì¬ ì—¬ë¶€ì™€ ë°ì´í„° í’ˆì§ˆ í™•ì¸

    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    result = {
        "sqm_present": "SQM" in df.columns,
        "stack_status_present": "Stack_Status" in df.columns,
        "sqm_calculated_count": 0,
        "stack_parsed_count": 0,
        "warnings": [],
        "column_count": len(df.columns),
    }

    if result["sqm_present"]:
        result["sqm_calculated_count"] = df["SQM"].notna().sum()
        sqm_percentage = (result["sqm_calculated_count"] / len(df) * 100) if len(df) > 0 else 0
        logger.info(
            f"[SUCCESS] SQM: {result['sqm_calculated_count']}ê°œ ê³„ì‚°ë¨ ({sqm_percentage:.1f}%)"
        )
        if result["sqm_calculated_count"] == 0:
            result["warnings"].append("SQM ì»¬ëŸ¼ì€ ì¡´ì¬í•˜ì§€ë§Œ ê³„ì‚°ëœ ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result["warnings"].append("[ERROR] SQM ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        logger.warning("SQM ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if result["stack_status_present"]:
        result["stack_parsed_count"] = df["Stack_Status"].notna().sum()
        stack_percentage = (result["stack_parsed_count"] / len(df) * 100) if len(df) > 0 else 0
        logger.info(
            f"[SUCCESS] Stack_Status: {result['stack_parsed_count']}ê°œ íŒŒì‹±ë¨ ({stack_percentage:.1f}%)"
        )
        if result["stack_parsed_count"] == 0:
            result["warnings"].append("Stack_Status ì»¬ëŸ¼ì€ ì¡´ì¬í•˜ì§€ë§Œ íŒŒì‹±ëœ ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result["warnings"].append("[ERROR] Stack_Status ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        logger.warning("Stack_Status ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    return result


def normalize_header_names_for_stage3(df: pd.DataFrame) -> pd.DataFrame:
    """
    Stage 2ì™€ Stage 3 ê°„ í—¤ë”ëª… ì°¨ì´ë¥¼ ì •ê·œí™” (Stage 3 ì „ìš©)

    ë³€í™˜:
    - "No" â†’ "no."
    - "wh handling" â†’ "wh_handling_legacy"
    - "site  handling" (ê³µë°± 2ê°œ) â†’ "site handling" (ê³µë°± 1ê°œ)
    - ì¤‘ë³µ "no" ì»¬ëŸ¼ ì œê±° (no.ì™€ noê°€ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    """
    renamed = {}

    for col in df.columns:
        if col == "No":
            renamed[col] = "no."
        elif col == "wh handling":
            renamed[col] = "wh_handling_legacy"
        elif col == "site  handling":
            renamed[col] = "site handling"

    if renamed:
        df = df.rename(columns=renamed)
        logger.info(f"[INFO] Stage 3 í—¤ë”ëª… ì •ê·œí™”: {len(renamed)}ê°œ ì»¬ëŸ¼ ë³€ê²½ë¨")
        for old, new in renamed.items():
            logger.info(f"  - '{old}' â†’ '{new}'")

    # ì¤‘ë³µ 'no' ì»¬ëŸ¼ ì œê±° (no.ì™€ noê°€ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    if "no" in df.columns and "no." in df.columns:
        df = df.drop(columns=["no"], errors="ignore")
        logger.info("[INFO] ì¤‘ë³µ 'no' ì»¬ëŸ¼ ì œê±° ì™„ë£Œ (no. ìœ ì§€)")

    return df


def normalize_header_names_for_stage2(df: pd.DataFrame) -> pd.DataFrame:
    """
    Stage 2 í—¤ë”ëª… ì •ê·œí™”

    ë³€í™˜:
    - "No" â†’ "no."
    - "site  handling" (ê³µë°± 2ê°œ) â†’ "site handling" (ê³µë°± 1ê°œ)
    - ì¤‘ë³µ "no" ì»¬ëŸ¼ ì œê±° (no.ì™€ noê°€ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    """
    renamed = {}

    for col in df.columns:
        if col == "No":
            renamed[col] = "no."
        elif col == "site  handling":
            renamed[col] = "site handling"

    if renamed:
        df = df.rename(columns=renamed)
        logger.info(f"[INFO] Stage 2 í—¤ë”ëª… ì •ê·œí™”: {len(renamed)}ê°œ ì»¬ëŸ¼ ë³€ê²½ë¨")
        for old, new in renamed.items():
            logger.info(f"  - '{old}' â†’ '{new}'")

    # ì¤‘ë³µ 'no' ì»¬ëŸ¼ ì œê±° (no.ì™€ noê°€ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    if "no" in df.columns and "no." in df.columns:
        df = df.drop(columns=["no"], errors="ignore")
        logger.info("[INFO] ì¤‘ë³µ 'no' ì»¬ëŸ¼ ì œê±° ì™„ë£Œ (no. ìœ ì§€)")

    return df


def analyze_header_compatibility(df: pd.DataFrame, is_stage2: bool = False) -> dict:
    """
    DataFrameì˜ í—¤ë” í˜¸í™˜ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤

    Args:
        df: ë¶„ì„í•  DataFrame
        is_stage2: Stage 2 ì¶œë ¥ì¸ ê²½ìš° True

    Returns:
        í˜¸í™˜ì„± ë¶„ì„ ê²°ê³¼
    """
    manager = get_header_manager()
    standard_order = STAGE2_HEADER_ORDER if is_stage2 else STANDARD_HEADER_ORDER

    # í—¤ë” ë³€í˜• ê°ì§€
    variations = manager.detect_header_variations(df)

    # ë§¤ì¹­ ë¶„ì„
    mapping = manager.match_columns_to_standard(list(df.columns), standard_order)

    analysis = {
        "total_columns": len(df.columns),
        "standard_columns": len(standard_order),
        "matched_columns": len(mapping),
        "matching_rate": (len(mapping) / len(df.columns) * 100) if len(df.columns) > 0 else 0,
        "variations_detected": len(variations),
        "unmatched_columns": [col for col in df.columns if col not in mapping],
        "header_variations": variations,
        "recommendations": [],
    }

    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    if analysis["matching_rate"] < 80:
        analysis["recommendations"].append(
            "í—¤ë” ë§¤ì¹­ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. í—¤ë”ëª…ì„ í‘œì¤€ í˜•ì‹ì— ë§ì¶° ìˆ˜ì •í•˜ì„¸ìš”."
        )

    if analysis["variations_detected"] > 0:
        analysis["recommendations"].append(
            f"{analysis['variations_detected']}ê°œì˜ í—¤ë” ë³€í˜•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ê´€ëœ ëª…ëª… ê·œì¹™ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        )

    if analysis["unmatched_columns"]:
        analysis["recommendations"].append(
            f"{len(analysis['unmatched_columns'])}ê°œì˜ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ì´ ìˆìŠµë‹ˆë‹¤: {analysis['unmatched_columns'][:5]}"
        )

    return analysis
